{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6066a20-69fe-47ca-8271-7a05c3c0c29c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6066a20-69fe-47ca-8271-7a05c3c0c29c",
    "outputId": "bfa6a73b-ba99-416a-a822-42104adeb27e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/maichi98/PythonM2-jour4.git\n",
    "%cd PythonM2-jour4/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf03e276-d84b-4442-91e0-39169717b41d",
   "metadata": {
    "id": "cf03e276-d84b-4442-91e0-39169717b41d"
   },
   "source": [
    "<h1 style=\"font-size:50px; text-align:center; padding:15px;\">\n",
    "    Deep Learning Tutorial - Segmentation of Brain MR Images\n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1aa947-3438-4a38-ae9b-eff6a535b000",
   "metadata": {
    "id": "6a1aa947-3438-4a38-ae9b-eff6a535b000"
   },
   "source": [
    "In this tutorial, we will explore how **deep learning models**, particularly the **U-Net architecture**, can be applied to **automatically** segment brain tumors, from MR images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd71ac8-daa7-442d-9340-71ad7a7d697a",
   "metadata": {
    "id": "bdd71ac8-daa7-442d-9340-71ad7a7d697a"
   },
   "source": [
    "Over the last decade, deep learning has revolutionized numerous fields, including **medical imaging**. These Deep learning solutions are increasingly being integrated into patient care, allieviating the workload of medical professionals and enhancing diagnostic accuracy.\n",
    "\n",
    "One standout application is **automatic segmentation** which has become mainstream in medical diagnostics and treatment planning. **Manual contouring** is not only **time consuming** but also **highly subjective**, leading to **inter-observer variability**â€”differences in segmentation results between radiologists.\n",
    "\n",
    "An objective delianation of the tumor by use of a AI model with soTA performance makes the longitudinal assessment of **tumor progression** and its **response to treatment** much more **robust**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad041cc0-2342-4eb7-b2bf-b5591235bace",
   "metadata": {
    "id": "ad041cc0-2342-4eb7-b2bf-b5591235bace"
   },
   "source": [
    "<h1 style=\"font-size:25px; padding:5px;\">\n",
    "   But What is segmentation ?\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5a3285-7c27-494d-9d20-1a71a2fba4f7",
   "metadata": {
    "id": "5a5a3285-7c27-494d-9d20-1a71a2fba4f7"
   },
   "source": [
    "Segmentation is a key task in medical image analysis, where an image is divided into meaningful regions, typically to identify structures like **organs**, or **tumors**. In **brain tumor segmentation**, the goal is to separate the tumor from healthy brain tissue in MRI scans.\n",
    "\n",
    "Unlike **classification** (which labels an entire image), or **detection** (which identifies object locations), segmentation provides a **pixel-wise mask**, which is crucial for treatment planning, and disease monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f47d31-39e1-47a3-b34f-55a7c3fb3ee2",
   "metadata": {
    "id": "e7f47d31-39e1-47a3-b34f-55a7c3fb3ee2"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/maichi98/PythonM2-jour4/main/resources/classification%20vs%20detection%20vs%20segmentation.jpeg\" alt=\"Classification vs Detection vs Segmentation\" width=\"1000px\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc5605-9e1b-4202-ab1d-5db7ad68a6fd",
   "metadata": {
    "id": "74bc5605-9e1b-4202-ab1d-5db7ad68a6fd"
   },
   "source": [
    "In image segmentation, a deep learning model learns to classify each pixel in an image as belonging to a specific category :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50327538-f632-4c87-bd1a-8eeaeba602c7",
   "metadata": {
    "id": "50327538-f632-4c87-bd1a-8eeaeba602c7"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/maichi98/PythonM2-jour4/main/resources/segmentation.png\" alt=\"Classification vs Detection vs Segmentation\" width=\"1000px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0a3764-ce32-47e8-b812-7e309a27d260",
   "metadata": {
    "id": "df0a3764-ce32-47e8-b812-7e309a27d260"
   },
   "source": [
    "<h1 style=\"font-size:25px; padding:5px;\">\n",
    "The BraTS Challenge: Brain Tumor Segmentation Benchmark ðŸ§ ðŸŽ¯\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e2a4d3-fa42-4f77-bdfb-5f04e8f67273",
   "metadata": {
    "id": "c1e2a4d3-fa42-4f77-bdfb-5f04e8f67273"
   },
   "source": [
    "The **Brain Tumor Segmentation (BraTS) Challenge** is an international competition designed to advance the development of **automated brain tumor segmentation models**. It provides researchers with standardized datasets and evaluation metrics to compare different deep-learning approaches.\n",
    "\n",
    "the labels L1, L2, and L4 refer to different types of tumor structures annotated in the MRI scans. Specifically:\n",
    "\n",
    "- **L1 (NCR/NET - Necrotic and Non-Enhancing Tumor Core)** : Represents the necrotic (dead) tumor core and the non-enhancing tumor regions.\n",
    "\n",
    "- **L2 (ED - Peritumoral Edema)** : Represents peritumoral edema, which is swelling around the tumor due to fluid accumulation.\n",
    "It often surrounds the tumor and appears as hyperintense regions in FLAIR sequences.\n",
    "- **L4 (ET - Enhancing Tumor)** : Represents the enhancing tumor region, which is the actively growing part of the tumor.\n",
    "It appears as a bright region in contrast-enhanced MRI (T1ce) scans.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49523ef-d5c6-4595-9617-7db911a19e96",
   "metadata": {
    "id": "e49523ef-d5c6-4595-9617-7db911a19e96"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/maichi98/PythonM2-jour4/main/resources/BRats.png\" alt=\"two different subjects from the BraTS dataset\" width=\"1000px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca32fc3-0151-4165-8182-31c01d22f8a1",
   "metadata": {
    "id": "dca32fc3-0151-4165-8182-31c01d22f8a1"
   },
   "source": [
    "## ðŸ§  Exercise 0: Visualizing a BraTS Patient's MRI Scans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b75087-17f1-43bb-9b6a-ba0745882c61",
   "metadata": {
    "id": "a7b75087-17f1-43bb-9b6a-ba0745882c61"
   },
   "source": [
    "- Load **multi-modal MRI scans** (T1, T1gd, T2, FLAIR) from a BraTS patient in the folder (example BraTS patients).\n",
    "- Extract an **axial-slice** (middle cross section).\n",
    "- Visualize different **MRI sequences side by side** along with the slice's corresponding mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee1df2-2386-40ea-abde-116457574cc0",
   "metadata": {
    "id": "10ee1df2-2386-40ea-abde-116457574cc0"
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Specify the name of the patient whose data we want to visualize :\n",
    "patient_name = \"BraTS19_CBICA_AAG_1\"\n",
    "\n",
    "# Create a dictionary to store the paths to the MRI scans for different imaging modalities\n",
    "# The keys are the modality names (t1, t1ce, t2, flair), and the values are the corresponding file paths\n",
    "dict_mri_paths = {\n",
    "    imaging: fr\"example BraTS patients/{patient_name}/{patient_name}_{imaging}.nii.gz\"\n",
    "    for imaging in .............  # Your code here\n",
    "}\n",
    "\n",
    "# Path to the segmentation mask file for the patient\n",
    "path_seg = fr\"example BraTS patients/{patient_name}/{patient_name}_seg.nii.gz\"\n",
    "\n",
    "# Load the MRI images for each modality using nibabel\n",
    "# The images are stored as NumPy arrays in a dictionary, with modality names as keys\n",
    "dict_mri_images = {\n",
    "    imaging: nib.load(dict_mri_paths[imaging]).get_fdata()\n",
    "    for imaging in [\"t1\", \"t1ce\", \"t2\", \"flair\"]\n",
    "}\n",
    "\n",
    "# Load the segmentation mask using nibabel\n",
    "# The mask is stored as a NumPy array\n",
    "img_seg =  .............  # Your code here\n",
    "\n",
    "# Select the slice index to visualize, for example 55\n",
    "slice_idx = 55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dcd63d-9b4c-4ed6-9121-73074afb633e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "41dcd63d-9b4c-4ed6-9121-73074afb633e",
    "outputId": "d75da18d-4da4-4deb-96c5-a90b7ee0068e"
   },
   "outputs": [],
   "source": [
    "# Plot the MRI slices and the Labels data :\n",
    "fig, axes = plt.subplots(1, 5, figsize=(18, 4))\n",
    "\n",
    "for i, imaging in enumerate([\"t1\", \"t1ce\", \"t2\", \"flair\"]):\n",
    "\n",
    "    axes[i].imshow(dict_mri_images[imaging][:, :, slice_idx].T, cmap=\"gray\")\n",
    "    axes[i].set_title(imaging.upper())\n",
    "\n",
    "axes[4].imshow(img_seg[:, :, slice_idx].T, cmap=\"gray\")\n",
    "axes[4].set_title(\"Labels\")\n",
    "\n",
    "plt.suptitle(\"BraTS MRI Modalities - Axial View\", fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c87d3b-4017-4bab-a790-265030a2ea5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "id": "b1c87d3b-4017-4bab-a790-265030a2ea5c",
    "outputId": "580a6027-7d25-4734-93a7-7357345d61cd"
   },
   "outputs": [],
   "source": [
    "# Plot the MRI slices with the Labels overlayed on the mri image :\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i, imaging in enumerate([\"t1\", \"t1ce\", \"t2\", \"flair\"]):\n",
    "\n",
    "    axes[i].imshow(dict_mri_images[imaging][:, :, slice_idx].T, cmap=\"gray\")\n",
    "    axes[i].imshow(img_seg[:, :, slice_idx].T, cmap=\"jet\", alpha=0.2)\n",
    "    axes[i].set_title(imaging.upper())\n",
    "\n",
    "plt.suptitle(\"BraTS MRI Modalities - Axial View\", fontsize=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15934a3-3aa7-4060-870a-9055d625f3a5",
   "metadata": {
    "id": "a15934a3-3aa7-4060-870a-9055d625f3a5"
   },
   "source": [
    "In this tutorial, we'll simplify the BraTS challenge, as we will attempt to predict the **L4 (ET - Enhancing Tumor)** based on the T1CE imaging, using the U-Net architecture :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7677fa-bf35-4a98-a63d-7cb79330b9eb",
   "metadata": {
    "id": "df7677fa-bf35-4a98-a63d-7cb79330b9eb"
   },
   "source": [
    "<h1 style=\"font-size:15px; padding:5px;\">\n",
    "   Dataset Details\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb855f87-38cd-4cbc-baec-82db2d0bcba7",
   "metadata": {
    "id": "bb855f87-38cd-4cbc-baec-82db2d0bcba7"
   },
   "source": [
    "The dataset used in this tutorial consists of T1CE MRI brain scans from 335 patients.\n",
    "\n",
    "- Each patient has multiple **2D slices** extracted from their **3D T1CE MRI scan**.\n",
    "\n",
    "- The naming convention (patient_XXX_YYYY.nii.gz) indicates:\n",
    "    - **XXX** â†’ Patient ID\n",
    "    - **YYYY** â†’ Slice number within that patientâ€™s scan.\n",
    "\n",
    "\n",
    "\n",
    "The dataset is organized into two main folders :\n",
    "\n",
    "- **Train** - Contains images and corresponding segmentation masks from **284** patients, for training the deep learning model.\n",
    "- **Test**  - Contains images and segmentation masks from **51** patients, for evaluating the model after training\n",
    "\n",
    "Each subset is further divided into:\n",
    "\n",
    "- Images: Contains 2D slices of the brain MRI scans.\n",
    "  \n",
    "- Masks: Contains the corresponding binary segmentation masks, where:\n",
    "    - 1 (White) represents the enhancing tumor region .\n",
    "    - 0 (Black) represents all tissue that is not enhancing tumor .\n",
    "\n",
    "â”œâ”€â”€ data\n",
    "        \n",
    "    â”œâ”€â”€ train\n",
    "    â”‚   â”œâ”€â”€ images\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_001_0001.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_001_0002.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ ...\n",
    "    â”‚   â”œâ”€â”€ masks\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_001_0001.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_001_0002.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ ...\n",
    "    â”œâ”€â”€ test\n",
    "    â”‚   â”œâ”€â”€ images\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_251_0001.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_251_0002.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ ...\n",
    "    â”‚   â”œâ”€â”€ masks\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_251_0001.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ patient_251_0002.nii.gz\n",
    "    â”‚   â”‚   â”œâ”€â”€ ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7668761f-6d06-4fb6-a20c-457b7222305f",
   "metadata": {
    "id": "7668761f-6d06-4fb6-a20c-457b7222305f"
   },
   "source": [
    "<h1 style=\"font-size:15px; padding:5px;\">\n",
    "   U-Net Architecture\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78847d14-c12b-4807-a665-323559ea387d",
   "metadata": {
    "id": "78847d14-c12b-4807-a665-323559ea387d"
   },
   "source": [
    "U-Net is a fully convolutional neural network (FCN) designed for image segmentation, **widely used in medical imaging**. It follows an encoder-bottleneck-decoder structure :\n",
    "\n",
    "- **Encoder (Contracting Path)**: Uses convolution and pooling layers to extract high-level features.\n",
    "\n",
    "- **Decoder (Expanding Path)**: Upsamples features and combines them with encoder outputs via skip connections to restore spatial details.\n",
    "\n",
    "- **Skip Connections:** Help retain fine details by directly passing features from encoder to decoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79779217-acd7-4487-9229-965e92452e81",
   "metadata": {
    "id": "79779217-acd7-4487-9229-965e92452e81"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/maichi98/PythonM2-jour4/main/resources/net.png\" alt=\"U-Net model structure\" width=\"800px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56485f19-ddc5-481d-a06a-5c681d3051cb",
   "metadata": {
    "id": "56485f19-ddc5-481d-a06a-5c681d3051cb"
   },
   "source": [
    "By the end of this tutorial, you will:\n",
    "\n",
    "- Learn about the U-Net architecture\n",
    "- Implement a U-Net architecture in python using PyTorch\n",
    "- Train and evaluate the segmentation performance using relevant metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94eb6d-8f2a-4d30-994c-8758197ecf10",
   "metadata": {
    "id": "0e94eb6d-8f2a-4d30-994c-8758197ecf10"
   },
   "source": [
    "## ðŸ§  Exercise 1: Split the train dataset into training images and validation images :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f572f-6ded-4b5f-a078-b9b1720459a3",
   "metadata": {
    "id": "3c9f572f-6ded-4b5f-a078-b9b1720459a3"
   },
   "source": [
    "When training a deep learning model, our goal is to create a model that performs well not just on the training data, but also on unseen data (test set).\n",
    "\n",
    "If we only evaluate performance on the training set, we risk **overfitting**, where the model memorizes the training data instead of learning meaningful patterns.\n",
    "\n",
    "To ensure our deep learning model generalizes well to new, unseen data, we need to split our training dataset into two subsets:\n",
    "\n",
    "- **Training Set**  â€“ Used to train the model.\n",
    "- **Validation Set**  â€“ Used to evaluate model performance during **training** .\n",
    "\n",
    "We will implement a **Patient-based Splitting** to ensure that all slices from a single patient belong **only to either training or validation**, not both :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a4d6bf-9da3-444c-a7e0-aa445e544cd8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25a4d6bf-9da3-444c-a7e0-aa445e544cd8",
    "outputId": "792fe968-116d-4cc5-99e4-d92a66b1f0b5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "# Get a list of all slice filenames in the training and test directories :\n",
    "list_original_train_slices = os.listdir(.............)  # Your code here\n",
    "list_test_slices = os.listdir(.............)  # Your code here\n",
    "\n",
    "# Extract unique patient IDs from the training slice filenames :\n",
    "# We use a set to ensure uniqueness and then convert it back to a list\n",
    "list_original_train_ids = list(set(\n",
    "    [slice_name.split(\"_\")[1] for slice_name in list_original_train_slices]\n",
    "))\n",
    "\n",
    "# Split the training patient IDs into training and validation sets\n",
    "# We use an 80/20 split (80% for training, 20% for validation)\n",
    "# random_state=42 ensures that the split is always the same for reproducibility\n",
    "list_train_ids, list_val_ids = train_test_split(\n",
    "    .............,  # Your code here\n",
    "    test_size=.............,\n",
    "    random_state=.............\n",
    "    )\n",
    "\n",
    "# Create lists of training and validation slice filenames based on the split patient IDs\n",
    "# We filter the original list of training slices to include only those belonging to the selected patient IDs\n",
    "list_train_slices = [slice_name for slice_name\n",
    "                     in list_original_train_slices\n",
    "                     if slice_name.split(\"_\")[1] in .............] # Your code here\n",
    "\n",
    "list_val_slices = [slice_name for slice_name\n",
    "                   in list_original_train_slices\n",
    "                   if slice_name.split(\"_\")[1] in .............] # Your code here\n",
    "\n",
    "# Print the number of slices in each set\n",
    "print(fr\"Number of slices in the training set : {len(list_train_slices)}\")\n",
    "print(fr\"Number of slices in the validation set : {len(list_val_slices)}\")\n",
    "print(fr\"Number of slices in the testing set : {len(list_test_slices)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ab8be6-a073-42a0-a655-742ae9fa9d0f",
   "metadata": {
    "id": "f8ab8be6-a073-42a0-a655-742ae9fa9d0f"
   },
   "source": [
    "## ðŸ§  Exercise 2: Build the U-Net model :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bMCMhI4RgY91",
   "metadata": {
    "id": "bMCMhI4RgY91"
   },
   "source": [
    "In this exercise, we will implement the 2D U-Net model using **PyTorch**, a popular deep learning framework.\n",
    "\n",
    "Use this diagram of the architecture of our U-Net to build the blocks of the U-Net  :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88125e17-83d1-4585-a112-d3d1067e3a3f",
   "metadata": {
    "id": "88125e17-83d1-4585-a112-d3d1067e3a3f"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/maichi98/PythonM2-jour4/main/resources/unet.png\" alt=\"U-Net model structure\" width=\"1000px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae65b1-6d9d-4f19-b835-33e9076cdc51",
   "metadata": {
    "id": "b4ae65b1-6d9d-4f19-b835-33e9076cdc51"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        \"\"\"\n",
    "        Initialize the U-Net model.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int, optional): Number of input channels (default: 1 for grayscale images).\n",
    "            out_channels (int, optional): Number of output channels (default: 1 for binary segmentation).\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # --------------------------\n",
    "        # Encoder (Contracting Path)\n",
    "        # --------------------------\n",
    "        # Series of convolutional blocks to extract features and downsample the input\n",
    "        self.enc1 = self.conv_block(..., ...)  # Input channels to 32 feature maps\n",
    "        self.enc2 = self.conv_block(..., ...)  # 32 feature maps to 64 feature maps\n",
    "        self.enc3 = self.conv_block(..., ...)  # 64 feature maps to 128 feature maps\n",
    "        self.enc4 = self.conv_block(..., ...)  # 128 feature maps to 256 feature maps\n",
    "        self.enc5 = self.conv_block(..., ...)  # 256 feature maps to 320 feature maps\n",
    "\n",
    "\n",
    "        # --------------------------\n",
    "        # Bottleneck\n",
    "        # --------------------------\n",
    "        # Bridge between the encoder and decoder, with 320 feature maps\n",
    "        self.bottleneck = self.conv_block(..., ...)\n",
    "\n",
    "        # --------------------------\n",
    "        # Decoder (Expanding Path)\n",
    "        # --------------------------\n",
    "        # Series of Transpose Convolution blocks to restore spatial details\n",
    "        # and combine features from the encoder through skip connections\n",
    "        self.up1 = nn.ConvTranspose2d(..., ..., kernel_size=2, stride=2)  # Upsample from 320 to 256 feature maps\n",
    "        self.dec1 = self.conv_block(..., ...)  # Concatenate features (256 + 256 = 512) and process with conv_block\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(..., ..., kernel_size=2, stride=2)  # Upsample from 256 to 128 feature maps\n",
    "        self.dec2 = self.conv_block(..., ...)  # Concatenate features (128 + 128 = 256) and process with conv_block\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(..., ..., kernel_size=2, stride=2)  # Upsample from 128 to 64 feature maps\n",
    "        self.dec3 = self.conv_block(..., ...)  # Concatenate features (64 + 64 = 128) and process with conv_block\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(..., ..., kernel_size=2, stride=2)  # Upsample from 64 to 32 feature maps\n",
    "        self.dec4 = self.conv_block(..., ...)  # Concatenate features (32 + 32 = 64) and process with conv_block\n",
    "\n",
    "        # --------------------------\n",
    "        # Output Layer\n",
    "        # --------------------------\n",
    "        # Final convolutional layer to produce the segmentation output\n",
    "        self.out = nn.Conv2d(..., out_channels, kernel_size=1)  # 32 feature maps to the desired output channels\n",
    "\n",
    "        # --------------------------\n",
    "        # MaxPooling Layer\n",
    "        # --------------------------\n",
    "        # Used for downsampling in the encoder path\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=..., stride=2)  # 2x2 max pooling with stride 2\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass of the U-Net model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor (image).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor (segmentation mask).\n",
    "        \"\"\"\n",
    "        # --------------------------\n",
    "        # Encoding Path\n",
    "        # --------------------------\n",
    "        # Pass the input through the encoder blocks, applying max pooling for downsampling\n",
    "        x1 = self.enc1(...)  # First encoder block\n",
    "        x2 = self.enc2(...)  # Second encoder block with max pooling\n",
    "        x3 = self.enc3(...)  # Third encoder block with max pooling\n",
    "        x4 = self.enc4(...)  # Fourth encoder block with max pooling\n",
    "        x5 = self.enc5(...)  # Fifth encoder block with max pooling\n",
    "\n",
    "        # --------------------------\n",
    "        # Bottleneck\n",
    "        # --------------------------\n",
    "        # Process the encoded features through the bottleneck block\n",
    "        x = self.bottleneck(...)\n",
    "\n",
    "        # --------------------------\n",
    "        # Decoding Path\n",
    "        # --------------------------\n",
    "        # Upsample the features, concatenate with skip connections from the encoder,\n",
    "        # and pass through the decoder blocks\n",
    "        x = self.up1(...)  # Upsample using transposed convolution\n",
    "        x = torch.cat((..., ...), dim=1)  # Concatenate with features from the encoder (skip connection)\n",
    "        x = self.dec1(...)  # Process through the first decoder block\n",
    "\n",
    "        x = self.up2(...)  # Upsample using transposed convolution\n",
    "        x = torch.cat((..., ...), dim=1)  # Concatenate with features from the encoder (skip connection)\n",
    "        x = self.dec2(...)  # Process through the second decoder block\n",
    "\n",
    "        x = self.up3(...)  # Upsample using transposed convolution\n",
    "        x = torch.cat((..., ...), dim=1)  # Concatenate with features from the encoder (skip connection)\n",
    "        x = self.dec3(...)  # Process through the third decoder block\n",
    "\n",
    "        x = self.up4(...)  # Upsample using transposed convolution\n",
    "        x = torch.cat((..., ...), dim=1)  # Concatenate with features from the encoder (skip connection)\n",
    "        x = self.dec4(...)  # Process through the fourth decoder block\n",
    "\n",
    "        # --------------------------\n",
    "        # Output Layer\n",
    "        # --------------------------\n",
    "        # Apply the final convolutional layer to produce the output segmentation mask\n",
    "        return self.out(...)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        Define a convolutional block, consisting of two convolutional layers,\n",
    "        batch normalization, and ReLU activation.\n",
    "\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "\n",
    "        Returns:\n",
    "            nn.Sequential: A sequential container of the layers in the convolutional block.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(..., ..., kernel_size=3, padding=1),  # Convolutional layer\n",
    "            nn.BatchNorm2d(out_channels),  # Batch normalization for stability\n",
    "            nn.ReLU(inplace=True),  # ReLU activation function\n",
    "            nn.Conv2d(..., ..., kernel_size=3, padding=1),  # Convolutional layer\n",
    "            nn.BatchNorm2d(out_channels),  # Batch normalization for stability\n",
    "            nn.ReLU(inplace=True)  # ReLU activation function\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000f2d73-45e0-49ce-81c2-cd2acf6b5184",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "000f2d73-45e0-49ce-81c2-cd2acf6b5184",
    "outputId": "0ddc6b2f-a659-4841-a647-f87c3b720041"
   },
   "outputs": [],
   "source": [
    "# Create the U-Net model\n",
    "model = ...\n",
    "\n",
    "# Determine the device to use for training (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(fr\"Device used : {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CydWEjdthCkN",
   "metadata": {
    "id": "CydWEjdthCkN"
   },
   "source": [
    "## ðŸ§  Exercise 3: Study of the architecture :\n",
    "\n",
    "\n",
    "In this exercise, we will explore the architecture of the model we just implemented. Understanding the architecture helps in knowing how the model transforms the input through various layers to produce the output.\n",
    "\n",
    "We will use `torchsummary` for this purpose. It gives a detailed view of the modelâ€™s layers, output shapes, and the number of parameters. What are the heaviest layers? How many parameters does the model have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8420f-8d1f-4a97-8a61-7c5cd8a0ad34",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "27e8420f-8d1f-4a97-8a61-7c5cd8a0ad34",
    "outputId": "c576dabd-4c7c-407a-f86d-267f2d351647"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Print a summary of the model's architecture\n",
    "summary(model, input_size=...)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06M0vhwthQb9",
   "metadata": {
    "id": "06M0vhwthQb9"
   },
   "source": [
    "## ðŸ§  Exercise 4: loss function and Optimizer\n",
    "\n",
    "For the loss function, we will use the BCEWithLogitsLoss function from torch, which applies **sigmoid activation** internally before computing **binary cross-entropy loss**. It is ideal for **binary classification** tasks, including **pixel-wise segmentation** in deep learning.\n",
    "\n",
    "\n",
    "For the optimizer, we will use **Adam**. The optimizer is responsible for updating the model's parameters during training, by propagating and derivating the error from the loss through all the layers. Adam is a popular optimizer, which is a variant of gradient descent. It is an adaptive learning rate optimization algorithm that's been designed specifically for training deep neural networks. The learning rate determines how fast the model learns. A low learning rate might result in slow learning, while a high learning rate might result in unstable training. Adam adapts the learning rate for each parameter individually, allowing the model to quickly learn the parameters for the most significant features, while slowly learning the parameters for the less significant ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b84ca-c715-4693-874d-bccf49927a73",
   "metadata": {
    "id": "0e5b84ca-c715-4693-874d-bccf49927a73"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn....()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim....(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-DGhTU54l5ab",
   "metadata": {
    "id": "-DGhTU54l5ab"
   },
   "source": [
    "## ðŸ§  Exercise 5: Create custom Dataset class\n",
    "\n",
    "In this exercise, we will create a custom dataset class for loading our MRI and ask pairs.\n",
    "\n",
    "\n",
    "With the pair image, mask, let's also return the slice filename :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1453433-6166-41fb-b5df-a371ee77d353",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f1453433-6166-41fb-b5df-a371ee77d353",
    "outputId": "7e4bb792-ff2e-4afd-9973-31b819645eb9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "\n",
    "class BratsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading BraTS MRI data.\n",
    "\n",
    "    Args:\n",
    "        dir_data (str): Path to the directory containing the data.\n",
    "        list_slices (list): List of slice filenames to include in the dataset.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, dir_data, list_slices, transform=None):\n",
    "\n",
    "        self.dir_data = dir_data\n",
    "        self.list_slices = list_slices\n",
    "        self.transform= transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_slices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a sample from the dataset at the given index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the sample to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the slice name, image tensor, and mask tensor.\n",
    "        \"\"\"\n",
    "         # Get the filename of the slice at the given index\n",
    "        slice_filename = self.list_slices[idx]\n",
    "        # Construct the path to the image file\n",
    "        path_image = os.path.join(...)\n",
    "        # Construct the path to the mask file\n",
    "        path_mask = os.path.join(...)\n",
    "\n",
    "        # Load the NIfTI files using nibabel\n",
    "        image_nii = nib.load(path_image)\n",
    "        image_data = image_nii.get_fdata().astype(np.float32)\n",
    "\n",
    "        mask_nii = nib.load(path_mask)\n",
    "        mask_data = mask_nii.get_fdata().astype(np.float32)\n",
    "        # Add a channel dimension to the image and mask data, its new shape becomes : [1, H, W]\n",
    "        # This is necessary for PyTorch models that expect input tensors with a channel dimension\n",
    "        image_data = np.expand_dims(image_data, axis=0)\n",
    "        mask_data = np.expand_dims(mask_data, axis=0)\n",
    "\n",
    "        # Convert the NumPy arrays to PyTorch tensors\n",
    "        image_tensor = torch.tensor(image_data, dtype=torch.float32)\n",
    "        mask_tensor = torch.tensor(mask_data, dtype=torch.float32)\n",
    "\n",
    "        # Optionally, apply any additional transforms :\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image_data, mask=mask_data)\n",
    "            image_data = augmented['image']\n",
    "            mask_data = augmented['mask']\n",
    "\n",
    "        # Return a dictionary containing the slice name, image tensor, and mask tensor\n",
    "        return {\n",
    "            \"slice_name\": ..., # Slice name without the file extension\n",
    "            \"image\": ..., # Image tensor\n",
    "            \"mask\": ... # Mask tensor\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M2waCIKWnB69",
   "metadata": {
    "id": "M2waCIKWnB69"
   },
   "source": [
    "## ðŸ§  Exercise 6:  Data Augmentation\n",
    "\n",
    "**Data augmentation** is a technique used to artificially increase the size and diversity of a training dataset by applying random transformations to input images. This helps improve model generalization and reduces overfitting.\n",
    "\n",
    "The plots demonstrate the effects of different data augmentation techniques on a sample brain MRI slie and its corresponding segmentation mask. We will be plotting\n",
    "\n",
    "- **Original Image** :\n",
    "\n",
    "Shows the original brain MRI slice with the segmentation mask overlayed.\n",
    "The MRI slice is displayed in grayscale using the cmap=\"gray\" argument, with the mask overlayed\n",
    "\n",
    "\n",
    "- **Horizontal Flip (p=1.0)** :\n",
    "\n",
    "Shows the effect of horizontal flipping on the image and mask.\n",
    "The A.HorizontalFlip(p=1.0) transform is used, which guarantees a horizontal flip with probability 1.0.\n",
    "\n",
    "\n",
    "- **Vertical Flip (p=1.0)** :\n",
    "\n",
    "Shows the effect of vertical flipping on the image and mask.\n",
    "The A.VerticalFlip(p=1.0) transform is used, which guarantees a vertical flip with probability 1.0.\n",
    "\n",
    "\n",
    "- **Random Rotate90 (p=1.0)** :\n",
    "\n",
    "Shows the effect of rotating the image and mask by a multiple of 90 degrees.\n",
    "The A.RandomRotate90(p=1.0) transform is used, which guarantees a rotation by 90, 180, or 270 degrees with probability 1.0.\n",
    "\n",
    "\n",
    "- **Pipeline Output (p=0.5)** :\n",
    "\n",
    "Shows the result of applying our composed training pipeline of augmentations with a probability of 0.5 for each transform.\n",
    "\n",
    "\n",
    "**How does the augmentation pipeline affect the original image and mask?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291a1344-f228-4069-a2e2-317b4ec8c1d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 813
    },
    "id": "291a1344-f228-4069-a2e2-317b4ec8c1d0",
    "outputId": "14d10541-3877-4c0c-df52-bcab0ed1bc80"
   },
   "outputs": [],
   "source": [
    "# Visualize data augmentation\n",
    "\n",
    "## Load an a slice from a patient as an example :\n",
    "example = \"patient_001_0100.nii.gz\"\n",
    "path_image, path_mask = f\"data/train/images/{example}\", f\"data/train/masks/{example}\"\n",
    "image_nii, mask_nii = nib.load(path_image), nib.load(path_mask)\n",
    "image = image_nii.get_fdata().astype(np.float32)\n",
    "mask = mask_nii.get_fdata().astype(np.float32)\n",
    "\n",
    "\n",
    "# Define individual transformations with probability p=1.0 for visualization purposes\n",
    "# This ensures that the transformations are always applied for demonstration\n",
    "hflip = A.HorizontalFlip(p=1.0)     # Horizontal flip transformation\n",
    "vflip = A.VerticalFlip(p=1.0)       # Vertical flip transformation\n",
    "rotate90 = A.RandomRotate90(p=1.0)  # Random 90-degree rotation transformation\n",
    "\n",
    "# Define the training augmentation pipeline\n",
    "# This pipeline combines multiple transformations with specified probabilities\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5)\n",
    "])\n",
    "\n",
    "# Apply each transformation individually to the example image and mask\n",
    "hflip_aug = hflip(image=image, mask=mask)\n",
    "img_hflip, mask_hflip = hflip_aug[\"image\"], hflip_aug[\"mask\"]\n",
    "\n",
    "vflip_aug = ...\n",
    "img_vflip, mask_vflip = ...\n",
    "\n",
    "rotate90_aug = ...\n",
    "img_rotate90, mask_rotate90 = ...\n",
    "\n",
    "# Apply the composed pipeline (output will vary because of randomness)\n",
    "pipeline_aug = ...\n",
    "img_pipeline, mask_pipeline = ...\n",
    "\n",
    "# Visualize the results with overlayed masks\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(image.T, cmap=\"gray\")\n",
    "axes[0].imshow(mask.T, cmap=\"jet\", alpha=0.5)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "\n",
    "axes[1].imshow(img_hflip.T, cmap=\"gray\")\n",
    "axes[1].imshow(mask_hflip.T, cmap=\"jet\", alpha=0.5)\n",
    "axes[1].set_title(\"Horizontal Flip (p=1.0)\")\n",
    "\n",
    "axes[2].imshow(img_vflip.T, cmap=\"gray\")\n",
    "axes[2].imshow(mask_vflip.T, cmap=\"jet\", alpha=0.5)\n",
    "axes[2].set_title(\"Vertical Flip (p=1.0)\")\n",
    "\n",
    "axes[3].imshow(img_rotate90.T, cmap=\"gray\")\n",
    "axes[3].imshow(mask_rotate90.T, cmap=\"jet\", alpha=0.5)\n",
    "axes[3].set_title(\"Random Rotate90 (p=1.0)\")\n",
    "\n",
    "axes[4].imshow(img_pipeline.T, cmap=\"gray\")\n",
    "axes[4].imshow(mask_pipeline.T, cmap=\"jet\", alpha=0.5)\n",
    "axes[4].set_title(\"Pipeline Output (p=0.5)\")\n",
    "\n",
    "# Hide the unused subplot\n",
    "axes[5].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2jne9mlQydL0",
   "metadata": {
    "id": "2jne9mlQydL0"
   },
   "source": [
    "## ðŸ§  Exercise 7:  Create Train and Validation datasets and dataloaders\n",
    "\n",
    "In this exercise, you will create custom **Dataset** and **DataLoader** objects for the training and validation datasets, incorporating the **data augmentation pipeline** you defined earlier. This is an essential step in preparing your data for training a deep learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c353d8-4698-4595-9b1e-bc04f11c1a9d",
   "metadata": {
    "id": "51c353d8-4698-4595-9b1e-bc04f11c1a9d"
   },
   "outputs": [],
   "source": [
    "# Path to the training data directory\n",
    "dir_train = ...\n",
    "\n",
    "# Create a BratsDataset object for the training set\n",
    "train_dataset = BratsDataset(\n",
    "    dir_data = ...,\n",
    "    list_slices=...,\n",
    "    transform=...\n",
    ")\n",
    "\n",
    "# Create a BratsDataset object for the validation set\n",
    "# Use the validation slice filenames and do not apply any augmentations\n",
    "val_dataset = BratsDataset(\n",
    "    dir_data = ...,\n",
    "    list_slices=...,\n",
    "    transform=...\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983ec71-e5f9-421d-bff3-159a6d2e9438",
   "metadata": {
    "id": "6983ec71-e5f9-421d-bff3-159a6d2e9438"
   },
   "outputs": [],
   "source": [
    "# Create DataLoader objects for the training and validation datasets\n",
    "# We'll use a batch size of 64 for both dataloaders\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,  # Training dataset\n",
    "    batch_size=64,  # Number of samples per batch\n",
    "    shuffle=True,  # Shuffle the data at the beginning of each epoch\n",
    "    num_workers=2  # Number of worker processes for data loading (adjust based on your system)\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    ...,  # Validation dataset\n",
    "    batch_size=...,  # Number of samples per batch\n",
    "    shuffle=False,  # Do not shuffle the validation data\n",
    "    num_workers=2  # Number of worker processes for data loading (adjust based on your system)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jIJssLIjy-15",
   "metadata": {
    "id": "jIJssLIjy-15"
   },
   "source": [
    "## ðŸ§  Exercise 8:  Train the model\n",
    "\n",
    "Train the model for one loop by completing the training loop's code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac2154-93c7-4a5a-a522-7e8b9884a40d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02ac2154-93c7-4a5a-a522-7e8b9884a40d",
    "outputId": "62314900-b7b2-4502-a74e-104fded13003"
   },
   "outputs": [],
   "source": [
    "import tqdm as tqdm\n",
    "\n",
    "# Set the number of training epochs\n",
    "num_epochs = 1\n",
    "\n",
    "# Initialize lists to store training and validation loss history\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# Initialize the best validation loss to a very large value (infinity)\n",
    "# This will be updated during training when a lower validation loss is achieved\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Iterate over the specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # --------------------------\n",
    "    # Training Phase\n",
    "    # --------------------------\n",
    "    # Set the model to training mode\n",
    "    # This enables certain layers like dropout and batch normalization that are only used during training\n",
    "    model.train()\n",
    "    # Initialize a list to store training losses for the current epoch\n",
    "    train_losses = []\n",
    "\n",
    "    # Create a progress bar using tqdm to track the training progress\n",
    "    with tqdm.tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        # Iterate over the training data in batches\n",
    "        for batch in train_dataloader:\n",
    "            # Move the input images and target masks to the appropriate device (GPU if available)\n",
    "            inputs, labels = ....to(device), ....to(device)\n",
    "\n",
    "            # Zero the gradients of the optimizer\n",
    "            # This is necessary before each backpropagation step to avoid accumulating gradients from previous iterations\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: Get the model's predictions for the input images\n",
    "            outputs = ...\n",
    "\n",
    "            # Compute the loss between the model's predictions and the target masks\n",
    "            loss = ...\n",
    "\n",
    "            # Backward pass: Calculate the gradients of the loss with respect to the model's parameters\n",
    "            loss.backward()\n",
    "            # Update the model's parameters based on the calculated gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Save the loss for the current batch and update the progress bar\n",
    "            train_losses.append(loss.item())  # Append the loss value to the list\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"training_loss\": loss.item()})\n",
    "\n",
    "    # Compute the average training loss for the epoch\n",
    "    avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "    # Add the average training loss to the loss history\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "\n",
    "    # --------------------------\n",
    "    # Validation Phase\n",
    "    # --------------------------\n",
    "    print(\"Validation in progress ...\")  # Print a message indicating the start of validation\n",
    "    # Set the model to evaluation mode\n",
    "    # This disables certain layers like dropout and batch normalization that are only used during training\n",
    "    model.eval()\n",
    "    # Initialize a list to store validation losses for the current epoch\n",
    "    val_losses = []\n",
    "\n",
    "    # Disable gradient computation during validation to save memory and speed up computation\n",
    "    with torch.no_grad():\n",
    "        # Create a progress bar using tqdm to track the validation progress\n",
    "        with tqdm.tqdm(total=len(val_dataloader), desc=f\"Validation Epoch {epoch + 1}/{num_epochs}\", unit=\"batch\") as pbar_val:\n",
    "            # Iterate over the validation data in batches\n",
    "            for batch in val_dataloader:\n",
    "                # Move the input images and target masks to the appropriate device (GPU if available)\n",
    "                inputs, labels = ...\n",
    "\n",
    "                # Forward pass: Get the model's predictions for the input images\n",
    "                outputs = ...\n",
    "\n",
    "                # Compute the loss between the model's predictions and the target masks\n",
    "                loss = ...\n",
    "\n",
    "                # Save the loss for the current batch and update the progress bar\n",
    "                val_losses.append(loss.item())  # Append the loss value to the list\n",
    "                pbar_val.update(1)\n",
    "                pbar_val.set_postfix({\"validation_loss\": loss.item()})\n",
    "\n",
    "    # Compute the average validation loss for the epoch\n",
    "    avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "    # Add the average validation loss to the loss history\n",
    "    val_loss_history.append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Save Model Checkpoints\n",
    "    # --------------------------\n",
    "    # Save the model's state dictionary after each epoch (last model)\n",
    "    torch.save(model.state_dict(), \"last_model.pth\")\n",
    "\n",
    "    # If the current validation loss is the best so far, save the model as the best model\n",
    "    if ... < ...:\n",
    "        best_val_loss = ...  # Update the best validation loss\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save the model's state dictionary\n",
    "        print(f\"Best model updated at epoch {epoch + 1} with validation loss {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LPp4ts9u1xq3",
   "metadata": {
    "id": "LPp4ts9u1xq3"
   },
   "source": [
    "## ðŸ§  Exercise 9:  Evaluate the one-epoch trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PIMpReR2yYm",
   "metadata": {
    "id": "7PIMpReR2yYm"
   },
   "source": [
    "To evaluate our model, we will use the **Dice Score**.\n",
    "\n",
    "The Dice score, is a statistical metric used to gauge the similarity between two sets. In the context of image segmentation, we use it to assess how well the predicted segmentation mask overlaps with the ground truth mask.\n",
    "\n",
    "\n",
    "Imagine you have two shapes, one representing the ground truth segmentation (where the tumor actually is) and the other representing the model's prediction. The Dice score essentially measures how much these two shapes overlap, relative to their total combined area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hrmCe25O3Lgm",
   "metadata": {
    "id": "hrmCe25O3Lgm"
   },
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"https://raw.githubusercontent.com/maichi98/PythonM2-jour4/main/resources/dice.png\" alt=\"Classification vs Detection vs Segmentation\" width=\"800px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kSBDcojz1oYy",
   "metadata": {
    "id": "kSBDcojz1oYy"
   },
   "source": [
    "In this exercise, we will first create the custom test dataset and loader,  then we will evaluate the predictions of our one-epoch trained model, by computing the **dice** per slice, then we will compute the dice per **Test patient** :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcfbd8-b574-4042-a0fa-e5b8dd341003",
   "metadata": {
    "id": "e9dcfbd8-b574-4042-a0fa-e5b8dd341003"
   },
   "outputs": [],
   "source": [
    "# Create the test dataset and dataloader\n",
    "\n",
    "# path to the test data directory\n",
    "dir_test = ...\n",
    "\n",
    "# Create a BratsDataset object for the test set\n",
    "# Use the test slice filenames and do not apply any augmentations\n",
    "test_dataset = BratsDataset(\n",
    "    dir_data=...,  # Path to the test data\n",
    "    list_slices=...,  # List of test slice filenames\n",
    "    transform=...  # No augmentations for the test set\n",
    ")\n",
    "\n",
    "# Create a DataLoader for the test dataset\n",
    "# Use a batch size of 1 and do not shuffle the data\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,  # Test dataset\n",
    "    batch_size=1,  # Batch size of 1 (process one slice at a time)\n",
    "    shuffle=False  # Do not shuffle the test data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb998e7-c173-4179-aeaf-bb7ffc29c5d5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6fb998e7-c173-4179-aeaf-bb7ffc29c5d5",
    "outputId": "1153cae3-5800-4c5a-de10-6f74379172d9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set a smoothing parameter (epsilon) to avoid division by zero when calculating the Dice score\n",
    "eps = 1e-6\n",
    "\n",
    "# Set the model in evaluation mode :\n",
    "...\n",
    "\n",
    "# Create a pandas DataFrame to store the results for each slice\n",
    "df_dice = pd.DataFrame(columns=[\"slice_name\", \"patient\", \"slice\", \"union\", \"intersection\", \"dice\"])\n",
    "\n",
    "\n",
    "# Disable gradient calculation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        # Get the slice name from the batch data\n",
    "        slice_name = batch[\"slice_name\"][0]\n",
    "\n",
    "        # Move input images and target masks to the device\n",
    "        inputs, labels = batch[\"image\"].to(device), batch[\"mask\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # Squeeze out the channel dimension: now (B, H, W)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        labels = labels.squeeze(1)\n",
    "\n",
    "        # Apply sigmoid activation to convert the model's raw outputs (logits) to probabilities\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        # Threshold the probabilities to obtain binary predictions (0 or 1)\n",
    "        # A threshold of 0.5 is commonly used for binary segmentation\n",
    "        outputs = (outputs > 0.5).float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        # Calculate the intersection and union of the predicted and ground truth masks\n",
    "        intersection = (outputs * labels).sum(dim=(1, 2))\n",
    "        union = outputs.sum(dim=(1, 2)) + labels.sum(dim=(1, 2))\n",
    "\n",
    "        intersection, union = intersection.item(), union.item()\n",
    "\n",
    "        # Compute Dice coefficient for this batch (or single sample, if batch_size==1)\n",
    "        dice = (((2.0 * intersection) + eps) / (union + eps))\n",
    "\n",
    "        print(fr\"Slice name : {slice_name} -- GT volume: {labels.sum().item()}mm^3 -- Dice score: {dice * 100:.2f} %\")\n",
    "        df_dice.loc[len(df_dice)] = {\n",
    "            \"slice_name\": slice_name, \"patient\": slice_name.split(\"_\")[1], \"slice\": slice_name.split(\"_\")[2],\n",
    "            \"union\": union, \"intersection\": intersection, \"dice\": dice\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc0890-0dcf-4170-bb66-b6ae21d3ddd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d5bc0890-0dcf-4170-bb66-b6ae21d3ddd3",
    "outputId": "6b59116d-be57-419c-9a4f-2bf1967a480c"
   },
   "outputs": [],
   "source": [
    "# Compute the average Dice score across all test slices :\n",
    "average_dice = ...\n",
    "print(f\"Average Dice score across all test slices: {average_dice * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VNw7WiioC7GG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNw7WiioC7GG",
    "outputId": "93b91201-ebee-49da-b494-b5e77858dce9"
   },
   "outputs": [],
   "source": [
    "# Group the DataFrame by patient\n",
    "patient_dice = df_dice.groupby(\"patient\")\n",
    "\n",
    "# Calculate the sum of intersections and unions for each patient\n",
    "patient_stats = patient_dice.agg({\"intersection\": \"sum\",\n",
    "                                  \"union\": \"sum\"})\n",
    "\n",
    "# Calculate the Dice score for each patient\n",
    "patient_stats[\"dice\"] = ...\n",
    "\n",
    "# Print the Dice score for each patient\n",
    "print(\"Dice scores per patient:\")\n",
    "print(patient_stats[[\"dice\"]].map(lambda x: f\"{x * 100:.2f}%\"))\n",
    "\n",
    "# Calculate the average Dice score across all patients\n",
    "average_patient_dice = patient_stats[\"dice\"].mean()\n",
    "print(f\"\\nAverage Dice score across all patients: {average_patient_dice * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9BP3mbauEXmd",
   "metadata": {
    "id": "9BP3mbauEXmd"
   },
   "source": [
    "## ðŸ§  Exercise 10:  Evaluate a fully-trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lDOGa1kREeNE",
   "metadata": {
    "id": "lDOGa1kREeNE"
   },
   "source": [
    "In this exercise, we will load a pre-trained model and evaluate the predictions of our one-epoch trained model, by computing the **dice** per slice, then we will compute the dice per **Test patient** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e520f-81f9-4170-bde4-d2af786419d9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "063e520f-81f9-4170-bde4-d2af786419d9",
    "outputId": "d0e986b9-91b4-4251-948e-db3b65619eee"
   },
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "\n",
    "# Determine the device to use for inference (GPU if available, otherwise CPU)\n",
    "device = ...\n",
    "\n",
    "# Create an instance of the U-Net model\n",
    "fully_trained_model = ...\n",
    "# Load the model's state dictionary from the saved file\n",
    "# 'best_model_12022025.pth' is the filename of the saved model\n",
    "fully_trained_model.load_state_dict(torch.load(..., map_location=device))\n",
    "\n",
    "# Move the model to the selected device (GPU or CPU)\n",
    "...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522c11e7-5f5c-4173-bd50-197ff19bb70a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "522c11e7-5f5c-4173-bd50-197ff19bb70a",
    "outputId": "58a1eb12-94c3-49f4-bd0e-d5fc9312a27e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fully_trained_model.eval()\n",
    "\n",
    "# Create a pandas DataFrame to store the results for each slice\n",
    "df_fully_trained_dice = pd.DataFrame(columns=[\"slice_name\", \"patient\", \"slice\", \"union\", \"intersection\", \"dice\"])\n",
    "\n",
    "\n",
    "# Disable gradient calculation during inference\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "\n",
    "        # Get the slice name from the batch data\n",
    "        slice_name = batch[\"slice_name\"][0]\n",
    "\n",
    "        # Move input images and target masks to the device\n",
    "        inputs, labels = batch[\"image\"].to(device), batch[\"mask\"].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = fully_trained_model(inputs)\n",
    "        # Squeeze out the channel dimension: now (B, H, W)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        labels = labels.squeeze(1)\n",
    "\n",
    "        # Apply sigmoid activation to convert the model's raw outputs (logits) to probabilities\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        # Threshold the probabilities to obtain binary predictions (0 or 1)\n",
    "        # A threshold of 0.5 is commonly used for binary segmentation\n",
    "        outputs = (outputs > 0.5).float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        # Calculate the intersection and union of the predicted and ground truth masks\n",
    "        intersection = (outputs * labels).sum(dim=(1, 2))\n",
    "        union = outputs.sum(dim=(1, 2)) + labels.sum(dim=(1, 2))\n",
    "\n",
    "        intersection, union = intersection.item(), union.item()\n",
    "\n",
    "        # Compute Dice coefficient for this batch (or single sample, if batch_size==1)\n",
    "        dice = (((2.0 * intersection) + eps) / (union + eps))\n",
    "\n",
    "        print(fr\"Slice name : {slice_name} -- GT volume: {labels.sum().item()}mm^3 -- Dice score: {dice * 100:.2f} %\")\n",
    "        df_fully_trained_dice.loc[len(df_fully_trained_dice)] = {\n",
    "            \"slice_name\": slice_name, \"patient\": slice_name.split(\"_\")[1], \"slice\": slice_name.split(\"_\")[2],\n",
    "            \"union\": union, \"intersection\": intersection, \"dice\": dice\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52c9a6d-33f7-489f-b92b-dc5f119a836b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d52c9a6d-33f7-489f-b92b-dc5f119a836b",
    "outputId": "53129d93-47ff-410f-bd46-1bfe03a461e5"
   },
   "outputs": [],
   "source": [
    "# Compute the average Dice score across all test slices :\n",
    "average_dice = df_fully_trained_dice[\"dice\"].mean()\n",
    "print(f\"Average Dice score across all test slices: {average_dice * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vT-POSzkGrfk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT-POSzkGrfk",
    "outputId": "614f1ae1-ba74-42f6-9b83-82876d624f85"
   },
   "outputs": [],
   "source": [
    "# Group the DataFrame by patient\n",
    "patient_dice = df_fully_trained_dice.groupby(\"patient\")\n",
    "\n",
    "# Calculate the sum of intersections and unions for each patient\n",
    "patient_stats = patient_dice.agg({\"intersection\": \"sum\",\n",
    "                                  \"union\": \"sum\"})\n",
    "\n",
    "# Calculate the Dice score for each patient\n",
    "patient_stats[\"dice\"] = ((2 * patient_stats[\"intersection\"]) + eps) / (patient_stats[\"union\"] + eps)\n",
    "\n",
    "# Print the Dice score for each patient\n",
    "print(\"Dice scores per patient:\")\n",
    "print(patient_stats[[\"dice\"]].map(lambda x: f\"{x * 100:.2f}%\"))\n",
    "\n",
    "# Calculate the average Dice score across all patients\n",
    "average_patient_dice = patient_stats[\"dice\"].mean()\n",
    "print(f\"\\nAverage Dice score across all patients: {average_patient_dice * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BwBbJ_FjIaTz",
   "metadata": {
    "id": "BwBbJ_FjIaTz"
   },
   "source": [
    "## ðŸ§  Exercise 11:  prediction plots\n",
    "\n",
    "In this exercise, let's plot some predictions from a patient with a high dice and from a patient with low dice ðŸ‡°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uPx1J0-2I-3K",
   "metadata": {
    "id": "uPx1J0-2I-3K"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def plot_slices(patient, n_slices, model, df_dice):\n",
    "    list_patient_slices = [s for s in list_test_slices if patient in s]\n",
    "\n",
    "    n_slices = min(n_slices, len(list_patient_slices))\n",
    "\n",
    "    # Randomly choose n_slices without picking the same slice twice.\n",
    "    list_patient_slices = random.sample(list_patient_slices, n_slices)\n",
    "\n",
    "    _test_dataset = BratsDataset(\n",
    "        dir_data=dir_test,\n",
    "        list_slices=list_patient_slices,\n",
    "        transform=None\n",
    "    )\n",
    "\n",
    "    _test_dataloader = DataLoader(_test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    root = int(np.sqrt(n_slices)) + 1\n",
    "\n",
    "    # Create a figure and a grid of subplots\n",
    "    fig = plt.figure(figsize=(5 * root, 5 * root))\n",
    "\n",
    "    for i, batch in enumerate(_test_dataloader):\n",
    "        slice_name = batch[\"slice_name\"][0]\n",
    "        inputs, labels = batch[\"image\"].to(device), batch[\"mask\"].to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.squeeze(1)\n",
    "        labels = labels.squeeze(1)\n",
    "\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        outputs = (outputs > 0.5).float()\n",
    "        labels = labels.float()\n",
    "\n",
    "        dice = df_dice[df_dice[\"slice_name\"] == slice_name][\"dice\"].values[0]\n",
    "\n",
    "        # Plot on the correct subplot\n",
    "        ax = fig.add_subplot(root, root, i + 1)\n",
    "\n",
    "        ax.imshow(inputs[0, 0, :, :].cpu().numpy().T, cmap=\"gray\")\n",
    "\n",
    "        # Display only labels with value 1 (tumor region)\n",
    "        masked_labels = np.ma.masked_where(labels[0, :, :].cpu().numpy().T == 0, labels[0, :, :].cpu().numpy().T)\n",
    "        ax.imshow(masked_labels, cmap=\"Blues\", alpha=1)\n",
    "\n",
    "        masked_outputs = np.ma.masked_where(outputs[0, :, :].cpu().numpy().T == 0, outputs[0, :, :].cpu().numpy().T)\n",
    "        ax.imshow(masked_outputs, cmap=\"jet\", alpha=0.4)\n",
    "\n",
    "        ax.set_title(f\"{slice_name} - Dice: {dice:.2f}\", fontsize=14)\n",
    "        ax.axis(\"off\")\n",
    "        ax.axis(\"off\")  # Turn off axis ticks and labels\n",
    "\n",
    "    plt.tight_layout()  # Adjust subplot spacing\n",
    "    plt.show()  # Display the figure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sK7lHXvfHcY7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sK7lHXvfHcY7",
    "outputId": "10e8cbc7-1124-4aa6-fc10-d1b390451b3c"
   },
   "outputs": [],
   "source": [
    "high_dice_patient = \"patient_006\"\n",
    "\n",
    "plot_slices(high_dice_patient, 16, fully_trained_model, df_fully_trained_dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KmM8WI4mLvwT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "KmM8WI4mLvwT",
    "outputId": "91c04678-413d-4fac-94c0-0686e13d778d"
   },
   "outputs": [],
   "source": [
    "low_dice_patient = \"patient_317\"\n",
    "\n",
    "plot_slices(low_dice_patient, 16, fully_trained_model, df_fully_trained_dice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PRHB8xiPUgHd",
   "metadata": {
    "id": "PRHB8xiPUgHd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
